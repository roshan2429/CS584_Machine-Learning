{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61a50bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing pandas librarry to get some information about dataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Loadinf the training  and testing data of Titanic datasets\n",
    "\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0298bb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n"
     ]
    }
   ],
   "source": [
    "#  exploring the dataset\n",
    "print(train_data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df24c0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(train_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed8ecbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping  unnecessary columns or handling  missing values in the dataset\n",
    "\n",
    "# here, dropping 'Name', 'Ticket', 'Cabin' columns which might not directly impact survival of the passangers.\n",
    "\n",
    "train_data.drop(['Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\n",
    "test_data.drop(['Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ae35825",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handling missing values (e.g., filling missing age values with median)\n",
    "    \n",
    "train_data['Age'].fillna(train_data['Age'].median(), inplace=True)\n",
    "test_data['Age'].fillna(test_data['Age'].median(), inplace=True)\n",
    "test_data['Fare'].fillna(test_data['Fare'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a941f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting  categorical variables into numerical ones (e.g., 'Sex', 'Embarked')\n",
    "train_data = pd.get_dummies(train_data, columns=['Sex', 'Embarked'])\n",
    "test_data = pd.get_dummies(test_data, columns=['Sex', 'Embarked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e889ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining  features and target variable\n",
    "\n",
    "features = train_data.drop('Survived', axis=1)\n",
    "target = train_data['Survived']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a40f8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initializing the model with random forest classifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Define the number of folds we want\n",
    "num_folds = 5\n",
    "\n",
    "# Initializing  KFolds\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73e13125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Accuracy: 0.8268156424581006\n",
      "Fold 2 Accuracy: 0.7921348314606742\n",
      "Fold 3 Accuracy: 0.848314606741573\n",
      "Fold 4 Accuracy: 0.7808988764044944\n",
      "Fold 5 Accuracy: 0.8258426966292135\n"
     ]
    }
   ],
   "source": [
    "# Perform cross-validation\n",
    "fold_accuracy = []\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(features, target)):\n",
    "    X_train, X_val = features.iloc[train_idx], features.iloc[val_idx]\n",
    "    y_train, y_val = target.iloc[train_idx], target.iloc[val_idx]\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on validation set\n",
    "    predictions = model.predict(X_val)\n",
    "\n",
    "    # Calculate accuracy for this fold\n",
    "    accuracy = accuracy_score(y_val, predictions)\n",
    "    fold_accuracy.append(accuracy)\n",
    "    print(f\"Fold {fold+1} Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a421c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db08e84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.8148013307388112\n"
     ]
    }
   ],
   "source": [
    "# Calculate average accuracy across all folds\n",
    "average_accuracy = sum(fold_accuracy) / len(fold_accuracy)\n",
    "print(f\"Average Accuracy: {average_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1cf399a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8212290502793296\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.89      0.85       105\n",
      "           1       0.82      0.73      0.77        74\n",
      "\n",
      "    accuracy                           0.82       179\n",
      "   macro avg       0.82      0.81      0.81       179\n",
      "weighted avg       0.82      0.82      0.82       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Splitting the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize a simple model (e.g., RandomForestClassifier)\n",
    "simple_model = RandomForestClassifier()\n",
    "\n",
    "# Train the model using the training set\n",
    "simple_model.fit(X_train, y_train)\n",
    "\n",
    "# Validate the model using the validation set\n",
    "val_predictions = simple_model.predict(X_val)\n",
    "\n",
    "# Evaluate the model's performance on the validation set\n",
    "val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "# Generate a classification report for detailed performance analysis\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, val_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9eb8bd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy with Selected Features: 0.8156424581005587\n"
     ]
    }
   ],
   "source": [
    "# Using RandomForestClassifier to get feature importances\n",
    "feature_importances = model.feature_importances_\n",
    "\n",
    "# Identify and select important features\n",
    "important_features = pd.Series(feature_importances, index=features.columns).sort_values(ascending=False)\n",
    "selected_features = important_features[:8].index  # Select top 8 important features\n",
    "\n",
    "# Retrain the model using selected features\n",
    "selected_features_model = RandomForestClassifier()\n",
    "selected_features_model.fit(X_train[selected_features], y_train)\n",
    "\n",
    "# Validate the model using the selected features\n",
    "val_predictions_selected = selected_features_model.predict(X_val[selected_features])\n",
    "\n",
    "# Evaluate the model's performance on the validation set\n",
    "val_accuracy_selected = accuracy_score(y_val, val_predictions_selected)\n",
    "print(f\"Validation Accuracy with Selected Features: {val_accuracy_selected}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c56be979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy with Regularization: 0.8044692737430168\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Apply regularization with a Logistic Regression model\n",
    "scaler = StandardScaler()\n",
    "regularized_model = make_pipeline(scaler, LogisticRegression(penalty='l1', solver='liblinear'))\n",
    "regularized_model.fit(X_train, y_train)\n",
    "\n",
    "# Validate the model using regularization\n",
    "val_predictions_regularized = regularized_model.predict(X_val)\n",
    "\n",
    "# Evaluate the model's performance on the validation set\n",
    "val_accuracy_regularized = accuracy_score(y_val, val_predictions_regularized)\n",
    "print(f\"Validation Accuracy with Regularization: {val_accuracy_regularized}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2608502a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy with GradientBoostingClassifier: 0.8156424581005587\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Experimenting with GradientBoostingClassifier\n",
    "gradient_boost_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gradient_boost_model.fit(X_train, y_train)\n",
    "\n",
    "# Validating the model using Gradient Boosting Classifier\n",
    "val_predictions_gradient_boost = gradient_boost_model.predict(X_val)\n",
    "\n",
    "# Evaluating the model's performance on the validation set\n",
    "val_accuracy_gradient_boost = accuracy_score(y_val, val_predictions_gradient_boost)\n",
    "print(f\"Validation Accuracy with GradientBoostingClassifier: {val_accuracy_gradient_boost}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7306f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy with SVM: 0.776536312849162\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Experimenting with Support Vector Machines\n",
    "svm_model = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Validate the model using Support Vector Machine\n",
    "val_predictions_svm = svm_model.predict(X_val)\n",
    "\n",
    "# Evaluate the model's performance on the validation data set\n",
    "val_accuracy_svm = accuracy_score(y_val, val_predictions_svm)\n",
    "print(f\"Validation Accuracy with SVM: {val_accuracy_svm}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "259b134a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85c2109e",
   "metadata": {},
   "outputs": [],
   "source": [
    "####1  Experiment 1: Feature Engineering and Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "add9cb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a 'FamilySize' feature by combining  SibSp and Parch\n",
    "train_data['FamilySize'] = train_data['SibSp'] + train_data['Parch']\n",
    "test_data['FamilySize'] = test_data['SibSp'] + test_data['Parch']\n",
    "\n",
    "# Drop SibSp and Parch as they are now redundant\n",
    "train_data.drop(['SibSp', 'Parch'], axis=1, inplace=True)\n",
    "test_data.drop(['SibSp', 'Parch'], axis=1, inplace=True)\n",
    "\n",
    "# Define features and target with new feature\n",
    "features_with_family = train_data.drop('Survived', axis=1)\n",
    "target = train_data['Survived']\n",
    "\n",
    "# Performing train-test split to split data\n",
    "X_train_with_family, X_val_with_family, y_train, y_val = train_test_split(features_with_family, target, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05c93731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest Accuracy: 0.8324022346368715\n",
      "LogisticRegression Accuracy: 0.7988826815642458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train and evaluate different models using new features\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(random_state=42),\n",
    "    'LogisticRegression': LogisticRegression(random_state=42)\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train_with_family, y_train)\n",
    "    predictions = model.predict(X_val_with_family)\n",
    "    accuracy = accuracy_score(y_val, predictions)\n",
    "    print(f\"{model_name} Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f4e87e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Experiment 2 Experiment 2: Hyperparameter Tuning and Ensemble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "361cdf0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest Tuned Accuracy: 0.8044692737430168\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Hyperparameter tuning for RandomForestClassifier\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(rf_model, param_grid, cv=5)\n",
    "grid_search.fit(X_train_with_family, y_train)\n",
    "\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "best_rf_model.fit(X_train_with_family, y_train)\n",
    "rf_predictions = best_rf_model.predict(X_val_with_family)\n",
    "rf_accuracy = accuracy_score(y_val, rf_predictions)\n",
    "print(f\"RandomForest Tuned Accuracy: {rf_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "204c6bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier Accuracy: 0.8044692737430168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Ensemble using VotingClassifier with the best-performing models\n",
    "voting_model = VotingClassifier(\n",
    "    estimators=[('RandomForest', best_rf_model), ('LogisticRegression', models['LogisticRegression'])],\n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "voting_model.fit(X_train_with_family, y_train)\n",
    "voting_predictions = voting_model.predict(X_val_with_family)\n",
    "voting_accuracy = accuracy_score(y_val, voting_predictions)\n",
    "print(f\"Voting Classifier Accuracy: {voting_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c751e71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
